{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "551d8b40-157d-4a1f-98e4-908b6bb7940a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connection to the Azure SQL Database\n",
    "\n",
    "Defined some variables to programmatically create the connection to the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a21a74-385b-4338-8c50-1498a9a25063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUsername = \"feuplogin\"\n",
    "jdbcPassword = \"Logproject33\"\n",
    "jdbcHostname = \"intranetfeupserver.database.windows.net\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"intranet14\"\n",
    "\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\": jdbcUsername,\n",
    "  \"password\": jdbcPassword,\n",
    "  \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636e288d-0ae6-461e-9680-b0cf6a6fde1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read and Transform Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3916e453-c9d5-4965-a5bf-e6e61325f7bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calendar Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c649e9b-e88f-4980-9805-26a6ba76c777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, sequence\n",
    "\n",
    "# read initial and final dates from database\n",
    "i_date = spark.read.jdbc(url=jdbcUrl, table=\"(SELECT TOP(1) [DataInicio] FROM [stg].[EXT_TBL_IMPUTACAO_DETALHE] ORDER BY [DataInicio] ASC) AS query\", properties=connectionProperties)\n",
    "f_date = spark.read.jdbc(url=jdbcUrl, table=\"(SELECT TOP(1) [DataFim] FROM [stg].[EXT_TBL_ORCAMENTO] ORDER BY [DataFim] DESC) AS query\", properties=connectionProperties)\n",
    "\n",
    "# extract initial and final dates as timestamps\n",
    "initial_date = expr(\"to_timestamp('{}')\".format(i_date.collect()[0][0]))\n",
    "final_date = expr(\"to_timestamp('{}')\".format(f_date.collect()[0][0]))\n",
    "\n",
    "# generate sequence of timestamps with 1 month interval\n",
    "timestamps_df = spark.range(1).select(sequence(initial_date, final_date, expr(\"interval 1 month\")).alias(\"timestamps\")).selectExpr(\"explode(timestamps) as timestamp\")\n",
    "\n",
    "# extract year, month and quarter from timestamp\n",
    "timestamps_df = timestamps_df.withColumn(\"Ano\", expr(\"year(timestamp)\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Mes\", expr(\"month(timestamp)\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Trimestre_Num\", expr(\"quarter(timestamp)\"))\n",
    "\n",
    "# create auxiliary columns\n",
    "timestamps_df = timestamps_df.withColumn(\"Ano_Mes_Num\", expr(\"concat_ws('-', Ano, LPAD(Mes, 2, '0'))\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Mes_Extenso\", expr(\"CASE Mes WHEN 1 THEN 'Janeiro' WHEN 2 THEN 'Fevereiro' WHEN 3 THEN 'Março' WHEN 4 THEN 'Abril' WHEN 5 THEN 'Maio' WHEN 6 THEN 'Junho' WHEN 7 THEN 'Julho' WHEN 8 THEN 'Agosto' WHEN 9 THEN 'Setembro' WHEN 10 THEN 'Outubro' WHEN 11 THEN 'Novembro' WHEN 12 THEN 'Dezembro' END\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Mes_Abrev\", expr(\"substring(Mes_Extenso, 1, 3)\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Mes_Abrev_E_Ano\", expr(\"concat(Mes_Abrev, ' ', Ano)\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Mes_Extenso_E_Ano\", expr(\"concat(Mes_Extenso, ' ', Ano)\"))\n",
    "timestamps_df = timestamps_df.withColumn(\"Trimestre\", expr(\"concat(Ano, ' Trimestre ', Trimestre_Num)\"))\n",
    "\n",
    "# select only needed columns\n",
    "result_df = timestamps_df.selectExpr(\"concat(Ano, LPAD(Mes, 2, '0')) as ID_Calendario\", \"Mes\", \"Mes_Abrev\", \"Mes_Extenso\", \"Ano\", \"Ano_Mes_Num\", \"Mes_Abrev_E_Ano\", \"Mes_Extenso_E_Ano\", \"Trimestre_Num\", \"Trimestre\")\n",
    "\n",
    "# insert data into database\n",
    "result_df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CALENDARIO]\", mode=\"overwrite\", properties=connectionProperties)\n",
    "\n",
    "# for invalid data\n",
    "table = [(-1, 0, '---', 'Não Definido', 0, '---', 'Não Definido', 'Não Definido', 0, 'Não Definido')]\n",
    "df = spark.createDataFrame(table, [\"ID_Calendario\", \"Mes\", \"Mes_Abrev\", \"Mes_Extenso\", \"Ano\", \"Ano_Mes_Num\", \"Mes_Abrev_E_Ano\", \"Mes_Extenso_E_Ano\", \"Trimestre_Num\", \"Trimestre\"])\n",
    "df.show()\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CALENDARIO]\", mode=\"append\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd261fde-d4e4-4fb6-ae74-f94dbc40f17a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Productivity Classification Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdd38bf-6ad4-470f-a26f-34d31e04a4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "classifications = ((-1, \"Inválido\", \"Inválida\", -1), \\\n",
    "    (0, \"[0%, 100%[\", \"Menos do Previsto\", 5), \\\n",
    "    (1, \"100%\", \"Como Previsto\", 4), \\\n",
    "    (2, \"]100%, 125%]\", \"Até 25% Horas Extra Consumidas\", 3), \\\n",
    "    (3, \"]125%, 150%]\", \"Até 50% Horas Extra Consumidas\", 2), \\\n",
    "    (4, \"]150%, 175%]\", \"Até 75% Horas Extra Consumidas\", 1), \\\n",
    "    (5, \"]175%, +∞]\", \"Mais de 75% Horas Extra Consumidas\", 0) \\\n",
    "  )\n",
    "columns = [\"ID_Classificacao_Produtividade\", \"Intervalo\", \"Nota\", \"Nota_Num\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = classifications, schema = columns)\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CLASSIFICACAO_PRODUTIVIDADE]\", mode=\"overwrite\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921721d6-b0c4-4780-a93a-091a995f1ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### State Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a63dea-c06d-4cce-9c2a-07b2fc2ce1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# Read distinct states from external table\n",
    "states_df = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Estado] from [stg].[EXT_TBL_PROJETOS]) AS query\", properties=connectionProperties)\n",
    "\n",
    "# Create data frame with 'Não Definido'\n",
    "df = spark.createDataFrame([(-1, 'Não Definido')], [\"ID_Estado\", \"Estado\"])\n",
    "\n",
    "# Union data frames\n",
    "df = df.union(states_df.selectExpr(\"row_number() over (order by Estado) as ID_Estado\", \"Estado\"))\n",
    "\n",
    "# Write data frame to destination table\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_ESTADO]\", mode=\"overwrite\", properties=connectionProperties)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5397bc07-e466-46ec-aa54-0f732a4c8f95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Profile Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb60e0e-63b7-4019-bb9d-0b314a8481b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring, col, when, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "profiles_font = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Perfil] from [stg].[EXT_TBL_ORCAMENTO]) AS query\", properties=connectionProperties)\n",
    "\n",
    "profiles = profiles_font.select(\"Perfil\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "sorted_profiles = sorted(profiles, key=lambda x: int(x[0:2]))\n",
    "\n",
    "profiles_df = spark.createDataFrame(sorted_profiles, \"string\").withColumnRenamed(\"value\", \"Nome_Perfil\")\n",
    "profiles_df = profiles_df.withColumn(\"Num_Perfil\", substring(col(\"Nome_Perfil\"), 1, 2).cast(\"int\"))\n",
    "profiles_df = profiles_df.withColumn(\"Posicao_Perfil\", when(profiles_df[\"Num_Perfil\"] < 10, substring(col(\"Nome_Perfil\"), 5, 50)).otherwise(substring(col(\"Nome_Perfil\"), 6, 50)))\n",
    "profiles_df = profiles_df.withColumn(\"ID_Perfil\", (1 + row_number().over(Window.orderBy(\"Nome_Perfil\"))))\n",
    "\n",
    "profiles_df = profiles_df.select(\"ID_Perfil\", \"Nome_Perfil\", \"Num_Perfil\", \"Posicao_Perfil\")\n",
    "\n",
    "profiles_df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PERFIL]\", mode=\"overwrite\", properties=connectionProperties)\n",
    "\n",
    "df = spark.createDataFrame([(-1, 'Não definido', 0, 'Não definida')], [\"ID_Perfil\", \"Nome_Perfil\", \"Num_Perfil\", \"Posicao_Perfil\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PERFIL]\", mode=\"append\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dc283e-b1e0-45e9-9e9e-d0eefc8045df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Employee Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3189d84e-825f-4290-a7cc-8565de2c777a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "employees_font = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Username] FROM [stg].[EXT_TBL_IMPUTACAO_DETALHE]) AS query\", properties=connectionProperties).select(\"Username\")\n",
    "employees_font.orderBy('Username')\n",
    "\n",
    "df = spark.createDataFrame([(-1, 'Não Definido')], [\"ID_Funcionario\", \"Nome_Funcionario\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_FUNCIONARIO]\", mode=\"overwrite\", properties=connectionProperties)\n",
    "\n",
    "df = employees_font.withColumn(\"ID_Funcionario\", monotonically_increasing_id() + 1)\n",
    "df = df.selectExpr(\"ID_Funcionario\", \"Username as Nome_Funcionario\")\n",
    "\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_FUNCIONARIO]\", mode=\"append\", properties=connectionProperties)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b15cb925-9c22-44ac-a32e-db9564beedbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d7ea0b-7c27-48b7-a135-c5725ea0eb69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, concat_ws, substring, col, lit, when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "tasks = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID], [CodigoProjeto], [Username], [Tarefa], [NHoras] from [stg].[EXT_TBL_HORASPREVISTAS]) AS query\", properties=connectionProperties)\n",
    "\n",
    "tasks = tasks.withColumn(\"employee_number\", substring(col(\"Username\"), 12, 50))\n",
    "\n",
    "project_task_hours = tasks.withColumn(\"Projeto_Funcionario_Horas\", concat_ws(\" \", col(\"CodigoProjeto\"),\n",
    "                                                                                  lit(\"-\"),\n",
    "                                                                                  lit(\"funcionário\"),\n",
    "                                                                                  col(\"employee_number\"),\n",
    "                                                                                  lit(\"-\"),\n",
    "                                                                                  concat(\n",
    "                                                                                      col(\"NHoras\").cast(\"int\"),\n",
    "                                                                                      when(col(\"NHoras\").cast(\"int\") == 1, lit(\" hora\")).otherwise(lit(\" horas\"))\n",
    "                                                                                  )\n",
    "                                                                            )\n",
    "                                      )\n",
    "\n",
    "tarefa_data = project_task_hours.selectExpr(\"ID as ID_Tarefa\", \"Tarefa as Nome_Tarefa\", \n",
    "                                            \"Projeto_Funcionario_Horas\", \"NHoras as Horas_Previstas_Tarefa\")\n",
    "\n",
    "tarefa_data.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_TAREFA]\", mode=\"overwrite\", properties=connectionProperties)\n",
    "\n",
    "df = spark.createDataFrame([(-1, 'Não Definido', 'Não Definido', 0)], \n",
    "                           [\"ID_Tarefa\", \"Nome_Tarefa\", \"Projeto_Funcionario_Horas\", \"Horas_Previstas_Tarefa\"])\n",
    "\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_TAREFA]\", mode=\"append\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd590b8e-7678-478d-a1c0-9d56181ed7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6964dcff-dc40-4750-9f08-1ba66ed01456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, sum, lit, coalesce\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "projects = spark.read.jdbc(url=jdbcUrl, \n",
    "                            table=\"(select P.[CodigoProjeto], [Departamento], [Area], sum(H.[NHoras]) as [HorasPrevistas] \\\n",
    "                                   from [stg].[EXT_TBL_PROJETOS] as P \\\n",
    "                                   join [stg].[EXT_TBL_HORASPREVISTAS] as H on P.[CodigoProjeto] = H.[CodigoProjeto] \\\n",
    "                                   group by P.[CodigoProjeto], [Departamento],[Area]) AS query\", \n",
    "                            properties=connectionProperties)\n",
    "\n",
    "projects = projects.withColumn(\"ID_Projeto\", monotonically_increasing_id()+1) \\\n",
    "             .withColumn(\"Codigo_Projeto\", col(\"CodigoProjeto\")) \\\n",
    "             .withColumn(\"Nome_Projeto\", concat(lit(\"Projeto \"), col(\"CodigoProjeto\"))) \\\n",
    "             .withColumn(\"Departamento\", coalesce(col(\"Departamento\"), lit(\"Não Especificado\"))) \\\n",
    "             .withColumn(\"Area\", coalesce(col(\"Area\"), lit(\"Não Especificada\"))) \\\n",
    "             .withColumn(\"Horas_Previstas_Projeto\", col(\"HorasPrevistas\")) \\\n",
    "             .select(\"ID_Projeto\", \"Codigo_Projeto\", \"Nome_Projeto\", \"Departamento\", \"Area\", \"Horas_Previstas_Projeto\")\n",
    "\n",
    "projects.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PROJETO]\", mode=\"overwrite\", properties=connectionProperties)\n",
    "\n",
    "df = spark.createDataFrame([(-1, 'Não Especificado', 'Não Especificado', 'Não Especificado', 'Não Especificada', 0)], \n",
    "                           [\"ID_Projeto\", \"Codigo_Projeto\", \"Nome_Projeto\", \"Departamento\", \"Area\", \"Horas_Previstas_Projeto\"])\n",
    "\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PROJETO]\", mode=\"append\", properties=connectionProperties)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f2ea917-ace3-4087-878a-3a37141c9f06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f97161-ad92-4f05-8885-d485acf43ade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  [0%, 100%[ - 5        - classification id 0\n",
    "  100% - 4              - classification id 1\n",
    "  ]100%, 125%] - 3      - classification id 2\n",
    "  ]125%, 150%] - 2      - classification id 3\n",
    "  ]150%, 175%] - 1      - classification id 4\n",
    "  ]175%, +∞] - 0        - classification id 5\n",
    "  '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, sum, lit, coalesce, monotonically_increasing_id\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# Read data from database\n",
    "imputations = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as Horas_Realizadas_Tarefa from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) AS query\", properties=connectionProperties)\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "get_hours_performed = \"(select [CodigoProjeto] as CodigoHoras, [FK_TarefaID] as TarefaIDHoras, sum([NHoras]) as Total_Horas_Realizadas_Tarefa_Query from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [FK_TarefaID]) AS QUERY\"\n",
    "\n",
    "hours_performed = spark.read.jdbc(url=jdbcUrl, table=get_hours_performed, properties=connectionProperties)\n",
    "imputations = imputations.join(hours_performed, (imputations.CodigoProjeto == hours_performed.CodigoHoras) & (imputations.FK_TarefaID == hours_performed.TarefaIDHoras), \"left\")\n",
    "imputations = imputations.withColumn(\"Total_Horas_Realizadas_Tarefa\", col(\"Total_Horas_Realizadas_Tarefa_Query\")).drop(\"CodigoHoras\", \"UsernameHoras\", \"TarefaIDHoras\", \"Total_Horas_Realizadas_Tarefa_Query\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Generate ID column\n",
    "imputations = imputations.withColumn(\"ID\", monotonically_increasing_id())\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Project ID\n",
    "project = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Projeto] as IDProjetoID, [Codigo_Projeto] from [dwProdutividade].[DIM_PROJETO]) AS query\", properties=connectionProperties)\n",
    "imputations = imputations.join(project, imputations.CodigoProjeto == project.Codigo_Projeto, \"left\")\n",
    "imputations = imputations.withColumn(\"ID_Projeto\", when(col(\"IDProjetoID\").isNull(), -1).otherwise(col(\"IDProjetoID\"))).drop(\"IDProjetoID\", \"Codigo_Projeto\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Calendar ID\n",
    "calendar = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Calendario] as IDCalendarioQuery, [Ano] as AnoQuery, [Mes] as MesQuery from [dwProdutividade].[DIM_CALENDARIO]) AS query\", properties=connectionProperties)\n",
    "imputations = imputations.join(calendar, (imputations.Ano == calendar.AnoQuery) & (imputations.Mes == calendar.MesQuery), \"left\")\n",
    "imputations = imputations.withColumn(\"ID_Calendario\", when(col(\"IDCalendarioQuery\").isNull(), -1).otherwise(col(\"IDCalendarioQuery\"))).drop(\"Ano\", \"Mes\", \"IDCalendarioQuery\", \"AnoQuery\", \"MesQuery\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Task ID\n",
    "task = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Tarefa] as IDTarefaQuery, [Nome_Tarefa], [Horas_Previstas_Tarefa] from [dwProdutividade].[DIM_TAREFA]) AS query\", properties=connectionProperties)\n",
    "imputations = imputations.join(task, imputations.FK_TarefaID == task.IDTarefaQuery, \"left\")\n",
    "imputations = imputations.withColumn(\"ID_Tarefa\", when(col(\"IDTarefaQuery\").isNull(), -1).otherwise(col(\"IDTarefaQuery\"))).drop(\"FK_TarefaID\", \"IDTarefaQuery\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Employee ID\n",
    "employee = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Funcionario] as IDFuncionarioQuery, [Nome_Funcionario] from [dwProdutividade].[DIM_FUNCIONARIO]) AS query\", properties=connectionProperties)\n",
    "imputations = imputations.join(employee, imputations.Username == employee.Nome_Funcionario, \"left\")\n",
    "imputations = imputations.withColumn(\"ID_Funcionario\", when(col(\"IDFuncionarioQuery\").isNull(), -1).otherwise(col(\"IDFuncionarioQuery\"))).drop(\"IDFuncionarioQuery\", \"Nome_Funcionario\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Profile ID\n",
    "profile_info = spark.read.jdbc(url=jdbcUrl, table=\"(select H.[ID] as IDTarefaHoras, H.[CodigoProjeto] as CodigoProjetoQuery, [Username] as UsernameQuery, H.[Tarefa] as TarefaQuery, [Perfil] as PerfilQuery from [stg].[EXT_TBL_HORASPREVISTAS] as H join [stg].[EXT_TBL_ORCAMENTO] as O on H.[OrcamentoID] = O.[ID]) AS query\", properties=connectionProperties)\n",
    "\n",
    "imputations = imputations.join(profile_info, (imputations.ID_Tarefa == profile_info.IDTarefaHoras) & (imputations.CodigoProjeto == profile_info.CodigoProjetoQuery) & (imputations.Nome_Tarefa == profile_info.TarefaQuery) & (imputations.Username == profile_info.UsernameQuery), \"left\")\n",
    "\n",
    "imputations.show()\n",
    "imputations = imputations.withColumn(\"Perfil\", col(\"PerfilQuery\")).drop(\"CodigoProjetoQuery\", \"Nome_Tarefa\", \"TarefaQuery\", \"UsernameQuery\", \"PerfilQuery\", \"IDCalendarioQuery\")\n",
    "imputations.show()\n",
    "\n",
    "profile = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Perfil] as IDPerfilQuery, [Nome_Perfil] from [dwProdutividade].[DIM_PERFIL]) AS query\", properties=connectionProperties)\n",
    "imputations = imputations.join(profile, (imputations.Perfil == profile.Nome_Perfil), \"left\")\n",
    "imputations.show()\n",
    "imputations = imputations.withColumn(\"ID_Perfil\", when(col(\"IDPerfilQuery\").isNull(), -1).otherwise(col(\"IDPerfilQuery\"))).drop(\"IDPerfilQuery\", \"Perfil\", \"Nome_Perfil\", \"CodigoProjeto\", \"Username\",)\n",
    "imputations.show()\n",
    "\n",
    "# Calculate Classification ID\n",
    "imputations = imputations.withColumn(\"ID_Classificacao_Produtividade_Tarefa\", when(col(\"Total_Horas_Realizadas_Tarefa\").isNull() , -1)\n",
    "                                                                      .when(col(\"Horas_Previstas_Tarefa\").isNull() , -1)\n",
    "                                                                      .when(col(\"Horas_Previstas_Tarefa\") == 0 , -1)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Tarefa\") / col(\"Horas_Previstas_Tarefa\") < 1, 0)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Tarefa\") / col(\"Horas_Previstas_Tarefa\") == 1, 1)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Tarefa\") / col(\"Horas_Previstas_Tarefa\") <= 1.25, 2)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Tarefa\") / col(\"Horas_Previstas_Tarefa\") <= 1.50, 3)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Tarefa\") / col(\"Horas_Previstas_Tarefa\") <= 1.75, 4)\n",
    "                                                                      .otherwise(5)).drop(\"Horas_Previstas_Tarefa\")\n",
    "\n",
    "imputations.show()\n",
    "\n",
    "# Write final output to database\n",
    "# Before writing it is necessary to rearrange the columns\n",
    "imputations = imputations.select(col(\"ID\"), col(\"Horas_Realizadas_Tarefa\"), col(\"ID_Projeto\"), col(\"ID_Calendario\"), col(\"ID_Tarefa\"), col(\"ID_Funcionario\"), col(\"ID_Perfil\"), col(\"ID_Classificacao_Produtividade_Tarefa\"))\n",
    "imputations.show()\n",
    "\n",
    "imputations.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[FACTO_TAREFA]\", mode=\"overwrite\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "575a492b-a23f-4851-974d-3fe05584da38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8f06c7-68aa-4035-84ee-408798e98db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  [0%, 100%[ - 5        - classification id 0\n",
    "  100% - 4              - classification id 1\n",
    "  ]100%, 125%] - 3      - classification id 2\n",
    "  ]125%, 150%] - 2      - classification id 3\n",
    "  ]150%, 175%] - 1      - classification id 4\n",
    "  ]175%, +∞] - 0        - classification id 5\n",
    "  '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, last, date_trunc, when, sum, lit, coalesce, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# Read data from database\n",
    "projects_mensal = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Ano], [Mes], sum([HorasRealizadas]) as Horas_Realizadas_Projeto from ( select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T group by [CodigoProjeto], [Ano], [Mes]) AS query\", properties=connectionProperties)\n",
    "\n",
    "''' for better visualization\n",
    "select [CodigoProjeto], [Ano], [Mes], sum([HorasRealizadas]) as Horas_Realizadas_Projeto \n",
    "from ( select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas \n",
    "  from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \n",
    "  group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T \n",
    "group by [CodigoProjeto], [Ano], [Mes]\n",
    "'''\n",
    "\n",
    "get_hours_performed = \"(select [CodigoProjeto] as CodigoHoras, sum([NHoras]) as Total_Horas_Realizadas_Projeto_Query from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto]) AS query\"\n",
    "hours_performed = spark.read.jdbc(url=jdbcUrl, table=get_hours_performed, properties=connectionProperties)\n",
    "\n",
    "projects_mensal = projects_mensal.join(hours_performed, projects_mensal.CodigoProjeto == hours_performed.CodigoHoras, \"left\")\n",
    "projects_mensal.show()\n",
    "projects_mensal = projects_mensal.withColumn(\"Total_Horas_Realizadas_Projeto\", (col(\"Total_Horas_Realizadas_Projeto_Query\"))).drop(\"Total_Horas_Realizadas_Projeto_Query\", \"CodigoHoras\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# Generate ID column\n",
    "projects_mensal = projects_mensal.withColumn(\"ID\", monotonically_increasing_id())\n",
    "projects_mensal.show()\n",
    "\n",
    "# Advance\n",
    "advance = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjecto], [DataAvanco], [Avanco] as Avanco_Projeto from [stg].[EXT_TBL_HISTORICO_AVANCOS]) AS query\", properties=connectionProperties)\n",
    "projects_mensal = projects_mensal.join(advance, (projects_mensal.CodigoProjeto == advance.CodigoProjecto) & (projects_mensal.Ano == year(advance.DataAvanco)) & (projects_mensal.Mes == month(advance.DataAvanco)), \"left\")\n",
    "projects_mensal.show(n = 50)\n",
    "projects_mensal_pandas = projects_mensal.toPandas()\n",
    "projects_mensal_pandas = projects_mensal_pandas.groupby([\"CodigoProjeto\", \"Ano\", \"Mes\"]).apply(lambda x: x.sort_values([\"Ano\", \"Mes\"]))\n",
    "projects_mensal_pandas['Avanco_Projeto'] = projects_mensal_pandas['Avanco_Projeto'].fillna(method = 'ffill')\n",
    "projects_mensal = spark.createDataFrame(projects_mensal_pandas)\n",
    "projects_mensal = projects_mensal.drop(\"CodigoProjecto\", \"DataAvanco\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# State \n",
    "state = spark.read.jdbc(url=jdbcUrl, table=\"(select [Estado], [CodigoProjeto] as CodigoProjetoQuery from [stg].[EXT_TBL_PROJETOS]) AS query\", properties=connectionProperties)\n",
    "projects_mensal = projects_mensal.join(state, projects_mensal.CodigoProjeto == state.CodigoProjetoQuery, \"left\")\n",
    "projects_mensal.show()\n",
    "state_info = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Estado] as IDEstadoQuery, [Estado] as EstadoQuery from [dwProdutividade].[DIM_ESTADO]) AS query\", properties=connectionProperties)\n",
    "projects_mensal = projects_mensal.join(state_info, projects_mensal.Estado == state_info.EstadoQuery, \"left\")\n",
    "projects_mensal.show()\n",
    "projects_mensal = projects_mensal.withColumn(\"ID_Estado\", when(col(\"IDEstadoQuery\").isNull(), -1).otherwise(col(\"IDEstadoQuery\"))).drop(\"CodigoProjetoQuery\", \"IDEstadoQuery\", \"EstadoQuery\", \"Estado\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# Calculate Project ID\n",
    "project = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Projeto] as IDProjetoID, [Codigo_Projeto], [Horas_Previstas_Projeto] from [dwProdutividade].[DIM_PROJETO]) AS query\", properties=connectionProperties)\n",
    "projects_mensal = projects_mensal.join(project, projects_mensal.CodigoProjeto == project.Codigo_Projeto, \"left\")\n",
    "projects_mensal.show()\n",
    "projects_mensal = projects_mensal.withColumn(\"ID_Projeto\", when(col(\"IDProjetoID\").isNull(), -1).otherwise(col(\"IDProjetoID\"))).drop(\"IDProjetoID\", \"Codigo_Projeto\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# Calculate Calendar ID\n",
    "calendar = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID_Calendario] as IDCalendarioQuery, [Ano] as AnoQuery, [Mes] as MesQuery from [dwProdutividade].[DIM_CALENDARIO]) AS query\", properties=connectionProperties)\n",
    "projects_mensal = projects_mensal.join(calendar, (projects_mensal.Ano == calendar.AnoQuery) & (projects_mensal.Mes == calendar.MesQuery), \"left\")\n",
    "projects_mensal.show()\n",
    "projects_mensal = projects_mensal.withColumn(\"ID_Calendario\", when(col(\"IDCalendarioQuery\").isNull(), -1).otherwise(col(\"IDCalendarioQuery\"))).drop(\"Ano\", \"Mes\", \"IDCalendarioQuery\", \"AnoQuery\", \"MesQuery\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# Calculate Classification ID\n",
    "projects_mensal = projects_mensal.withColumn(\"ID_Classificacao_Produtividade_Projeto\", when(col(\"Total_Horas_Realizadas_Projeto\").isNull() , -1)\n",
    "                                                                      .when(col(\"Horas_Previstas_Projeto\").isNull() , -1)\n",
    "                                                                      .when(col(\"Horas_Previstas_Projeto\") == 0 , -1)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Projeto\") / col(\"Horas_Previstas_Projeto\") < 1, 0)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Projeto\") / col(\"Horas_Previstas_Projeto\") == 1, 1)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Projeto\") / col(\"Horas_Previstas_Projeto\") <= 1.25, 2)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Projeto\") / col(\"Horas_Previstas_Projeto\") <= 1.50, 3)\n",
    "                                                                      .when(col(\"Total_Horas_Realizadas_Projeto\") / col(\"Horas_Previstas_Projeto\") <= 1.75, 4)\n",
    "                                                                      .otherwise(5)).drop(\"Horas_Previstas_Projeto\")\n",
    "projects_mensal.show()\n",
    "\n",
    "# Write final output to database\n",
    "projects_mensal = projects_mensal.select(col(\"ID\"), col(\"Avanco_Projeto\"), col(\"Horas_Realizadas_Projeto\"), col(\"ID_Estado\"), col(\"ID_Projeto\"), col(\"ID_Calendario\"), col(\"ID_Classificacao_Produtividade_Projeto\"))\n",
    "projects_mensal.show()\n",
    "\n",
    "projects_mensal.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[FACTO_PROJETO]\", mode=\"overwrite\", properties=connectionProperties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4084851272068028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
