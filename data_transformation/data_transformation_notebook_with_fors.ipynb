{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "551d8b40-157d-4a1f-98e4-908b6bb7940a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connection to the Azure SQL Database\n",
    "\n",
    "Defined some variables to programmatically create the connection to the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a21a74-385b-4338-8c50-1498a9a25063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUsername = \"feuplogin\"\n",
    "jdbcPassword = \"Logproject33\"\n",
    "jdbcHostname = \"intranet14.database.windows.net\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"intranetfeupp14\"\n",
    "\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\": jdbcUsername,\n",
    "  \"password\": jdbcPassword,\n",
    "  \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636e288d-0ae6-461e-9680-b0cf6a6fde1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read and Transform Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3916e453-c9d5-4965-a5bf-e6e61325f7bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calendar Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c649e9b-e88f-4980-9805-26a6ba76c777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "import datetime\n",
    "from pyspark.sql.functions import expr, sequence\n",
    "\n",
    "# to have the months in portuguese - locale unsuported for having the months in portuguese\n",
    "#import locale\n",
    "#locale.setlocale(locale.LC_TIME, 'pt_PT.utf8')\n",
    "\n",
    "i_date = spark.read.jdbc(url=jdbcUrl, table=\"(SELECT TOP(1) [DataInicio] FROM [stg].[EXT_TBL_IMPUTACAO_DETALHE] ORDER BY [DataInicio] ASC) AS query\", properties=connectionProperties)\n",
    "\n",
    "f_date = spark.read.jdbc(url=jdbcUrl, table=\"(SELECT TOP(1) [DataFim] FROM [stg].[EXT_TBL_ORCAMENTO] ORDER BY [DataFim] DESC) AS query\", properties=connectionProperties)\n",
    "\n",
    "initial_date = expr(\"to_timestamp('{}')\".format(i_date.collect()[0][0]))\n",
    "final_date = expr(\"to_timestamp('{}')\".format(f_date.collect()[0][0]))\n",
    "\n",
    "timestamps_array = sequence(initial_date, final_date, expr(\"interval 1 month\")).alias(\"timestamps\")\n",
    "\n",
    "for row in spark.range(1).select(timestamps_array).collect():\n",
    "    for timestamp in row.timestamps:\n",
    "        if (timestamp.month >= 10):\n",
    "            id = str(timestamp.year) + str(timestamp.month)\n",
    "            id = int(id)\n",
    "            year_month_num = str(timestamp.year) +\"-\" + str(timestamp.month)\n",
    "        else:\n",
    "            id = str(timestamp.year) + \"0\" + str(timestamp.month)\n",
    "            id = int(id)\n",
    "            year_month_num = str(timestamp.year) +\"-0\" + str(timestamp.month)\n",
    "            \n",
    "        #month_extensive = timestamp.strftime('%B') formats the date for just the month\n",
    "        if timestamp.month == 1:\n",
    "          month_extensive = 'Janeiro'\n",
    "        elif timestamp.month == 2:\n",
    "          month_extensive = 'Fevereiro'\n",
    "        elif timestamp.month == 3:\n",
    "          month_extensive = 'Março'\n",
    "        elif timestamp.month == 4:\n",
    "          month_extensive = 'Abril'\n",
    "        elif timestamp.month == 5:\n",
    "          month_extensive = 'Maio'\n",
    "        elif timestamp.month == 6:\n",
    "          month_extensive = 'Junho'\n",
    "        elif timestamp.month == 7:\n",
    "          month_extensive = 'Julho'\n",
    "        elif timestamp.month == 8:\n",
    "          month_extensive = 'Agosto'\n",
    "        elif timestamp.month == 9:\n",
    "          month_extensive = 'Setembro'\n",
    "        elif timestamp.month == 10:\n",
    "          month_extensive = 'Outubro'\n",
    "        elif timestamp.month == 11:\n",
    "          month_extensive = 'Novembro'\n",
    "        elif timestamp.month == 12:\n",
    "          month_extensive = 'Dezembro'\n",
    "\n",
    "        month_abbreviated = month_extensive[:3]\n",
    "        month_abbreviated_year = month_abbreviated + \" \" + str(timestamp.year)\n",
    "        month_extensive_year = month_extensive + \" \" + str(timestamp.year)\n",
    "\n",
    "        month_to_quarter = {1:1, 2:1, 3:1, 4:2, 5:2, 6:2, 7:3, 8:3, 9:3, 10:4, 11:4, 12:4}\n",
    "        quarter = month_to_quarter.pop(timestamp.month)\n",
    "        quarter_extensive = str(timestamp.year) + \" Trimestre \" + str(quarter)\n",
    "        \n",
    "        year_string = str(timestamp.year)\n",
    "        month_string = str(timestamp.month)\n",
    "        \n",
    "        # Creates the dataframe with the data needed\n",
    "        table = [(id, timestamp.month, month_abbreviated, month_extensive, timestamp.year, year_month_num, month_abbreviated_year, month_extensive_year, quarter, quarter_extensive)]\n",
    "        df = spark.createDataFrame(table, [\"ID_Calendario\", \"Mes\", \"Mes_Abrev\", \"Mes_Extenso\", \"Ano\", \"Ano_Mes_Num\", \"Mes_Abrev_E_Ano\", \"Mes_Extenso_E_Ano\", \"Trimestre_Num\", \"Trimestre\"])\n",
    "    \n",
    "        print(str(id) + \" | \" + str(timestamp.month) + \" | \" + month_abbreviated + \" | \" + month_extensive + \" | \" + str(timestamp.year) + \" | \" + str(year_month_num) + \" | \" + month_abbreviated_year + \" | \" + month_extensive_year + \" | \" + str(quarter) + \" | \" + quarter_extensive)\n",
    "\n",
    "        # Uploads the data to the dimension\n",
    "        df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CALENDARIO]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "# for tasks without time designated\n",
    "\n",
    "table = [(0, 0, '---', 'Não Definido', 0, '---', 'Não Definido', 'Não Definido', 0, 'Não Definido')]\n",
    "df = spark.createDataFrame(table, [\"ID_Calendario\", \"Mes\", \"Mes_Abrev\", \"Mes_Extenso\", \"Ano\", \"Ano_Mes_Num\", \"Mes_Abrev_E_Ano\", \"Mes_Extenso_E_Ano\", \"Trimestre_Num\", \"Trimestre\"])\n",
    "    \n",
    "print(str(id) + \" | \" + str(month) + \" | \" + month_abbreviated + \" | \" + month_extensive + \" | \" + str(year) + \" | \" + str(year_month_num) + \" | \" + month_abbreviated_year + \" | \" + month_extensive_year + \" | \" + str(quarter) + \" | \" + quarter_extensive)\n",
    "\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CALENDARIO]\", mode=\"append\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd261fde-d4e4-4fb6-ae74-f94dbc40f17a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Productivity Classification Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdd38bf-6ad4-470f-a26f-34d31e04a4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "\n",
    "intervals = [\"[0%, 100%[\", \"100%\", \"]100%, 125%]\", \"]125%, 150%]\", \"]150%, 175%]\", \"]175%, +∞]\"]\n",
    "notes = [\"Menos do Previsto\", \"Como Previsto\", \"Até 25% Horas Extra Consumidas\", \"Até 50% Horas Extra Consumidas\", \"Até 75% Horas Extra Consumidas\", \"Mais de 75% Horas Extra Consumidas\"]\n",
    "number_notes = [5, 4, 3, 2, 1, 0]\n",
    "\n",
    "for i in range(6):\n",
    "  df = spark.createDataFrame([(i, intervals[i], notes[i], number_notes[i])], [\"ID_Classificacao_Produtividade\", \"Intervalo\", \"Nota\", \"Nota_Num\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CLASSIFICACAO_PRODUTIVIDADE]\", mode=\"append\", properties=connectionProperties)\n",
    "  print(str(i) + \" | \" + intervals[i] + \" | \" + notes[i] + \" | \" + str(number_notes[i]))\n",
    "\n",
    "  i += 1\n",
    "\n",
    "df = spark.createDataFrame([(6, 'Inválido', 'Inválida', -1)], [\"ID_Classificacao_Produtividade\", \"Intervalo\", \"Nota\", \"Nota_Num\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_CLASSIFICACAO_PRODUTIVIDADE]\", mode=\"append\", properties=connectionProperties)\n",
    "print(str(6) + \" | \" + 'Inválido' + \" | \" + 'Inválida' + \" | \" + str(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921721d6-b0c4-4780-a93a-091a995f1ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### State Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a63dea-c06d-4cce-9c2a-07b2fc2ce1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "  \n",
    "states_font = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Estado] from [stg].[EXT_TBL_PROJETOS]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "states = []\n",
    "\n",
    "for row in states_font:\n",
    "    if row['Estado'] not in states:\n",
    "        states.append(row['Estado'])\n",
    "        \n",
    "df = spark.createDataFrame([(0, 'Não Definido')], [\"ID_Estado\", \"Estado\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_ESTADO]\", mode=\"append\", properties=connectionProperties)\n",
    "print(\"0 | Não Definido\")\n",
    "\n",
    "for i in range(len(states)):   \n",
    "  df = spark.createDataFrame([(i + 1, states[i])], [\"ID_Estado\", \"Estado\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_ESTADO]\", mode=\"append\", properties=connectionProperties)\n",
    "  print(str(i + 1) + \" | \" + states[i])\n",
    "  \n",
    "  i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5397bc07-e466-46ec-aa54-0f732a4c8f95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Profile Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb60e0e-63b7-4019-bb9d-0b314a8481b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "profiles_font = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Perfil] from [stg].[EXT_TBL_ORCAMENTO]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for row in profiles_font:\n",
    "    if row['Perfil'] not in profiles:\n",
    "        profiles.append(row['Perfil'])\n",
    "      \n",
    "# Sorts profiles by number\n",
    "sorted_profiles = sorted(profiles, key=lambda x: int(x[0:2]))\n",
    "  \n",
    "df = spark.createDataFrame([(0, 'Não definido', 0, 'Não definida')], [\"ID_Perfil\", \"Nome_Perfil\", \"Num_Perfil\", \"Posicao_Perfil\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PERFIL]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "i = 1\n",
    "for profile in sorted_profiles:\n",
    "  if int(profile[0:2]) in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    print(str(i) + \" | \" + profile + \" | \" + str(int(profile[0:2])) + \" | \" + profile[4:])\n",
    "    \n",
    "    df = spark.createDataFrame([(i, profile, int(profile[0:2]), profile[4:])], [\"ID_Perfil\", \"Nome_Perfil\", \"Num_Perfil\", \"Posicao_Perfil\"])\n",
    "    df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PERFIL]\", mode=\"append\", properties=connectionProperties)\n",
    "    \n",
    "  else:\n",
    "    print(str(i) + \" | \" + profile + \" | \" + str(int(profile[0:2])) + \" | \" + profile[5:])\n",
    "    \n",
    "    df = spark.createDataFrame([(i, profile, int(profile[0:2]), profile[5:])], [\"ID_Perfil\", \"Nome_Perfil\", \"Num_Perfil\", \"Posicao_Perfil\"])\n",
    "    df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PERFIL]\", mode=\"append\", properties=connectionProperties)\n",
    "  \n",
    "  i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dc283e-b1e0-45e9-9e9e-d0eefc8045df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Employee Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3189d84e-825f-4290-a7cc-8565de2c777a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "employees_font = spark.read.jdbc(url=jdbcUrl, table=\"(select distinct [Username] FROM [stg].[EXT_TBL_IMPUTACAO_DETALHE]) AS query\", properties=connectionProperties).collect()       # already returns the usernames sorted by the identifier number (ex: utilizador.231 - ordered by 231)\n",
    "\n",
    "employees = []\n",
    "\n",
    "for row in employees_font:\n",
    "    if row['Username'] not in employees:\n",
    "        employees.append(row['Username'])\n",
    "  \n",
    "df = spark.createDataFrame([(0, 'Não Definido')], [\"ID_Funcionario\", \"Nome_Funcionario\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_FUNCIONARIO]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "i = 1\n",
    "for employee in employees:\n",
    "  print(str(i) + \" | \" + employee)\n",
    "\n",
    "  df = spark.createDataFrame([(i, employee)], [\"ID_Funcionario\", \"Nome_Funcionario\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_FUNCIONARIO]\", mode=\"append\", properties=connectionProperties)\n",
    "  \n",
    "  i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b15cb925-9c22-44ac-a32e-db9564beedbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d7ea0b-7c27-48b7-a135-c5725ea0eb69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "tasks = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID], [CodigoProjeto], [Username], [Tarefa], [NHoras] from [stg].[EXT_TBL_HORASPREVISTAS]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "'''\n",
    "ids = tasks[i][0]\n",
    "projects = tasks[i][1]\n",
    "tasks = tasks[i][2]\n",
    "expected_hours = tasks[i][3]'''\n",
    "\n",
    "for task in tasks:\n",
    "  #print(str(task[0]) + \" | \" + task[1] + \" | \" + task[2] + \" | \" + str(int(task[3])))\n",
    "  \n",
    "  id = task[0]\n",
    "  \n",
    "  project = task[1]\n",
    "  \n",
    "  employee_name = task[2]\n",
    "  employee_number = task[2][11:]\n",
    "  \n",
    "  task_name = task[3]\n",
    "  \n",
    "  expected_hours = int(task[4])\n",
    "  \n",
    "  #project_task_hours = project + \"_employee\" + employee_number + \"_\" + str(expected_hours) + \"hours\"\n",
    "  project_task_hours = project + \" funcionário \" + employee_number + \" (\" + str(expected_hours) + \" horas)\"\n",
    "  \n",
    "  \n",
    "  print(str(id) + \" | \" + project + \" | \" + employee_name + \" | \" + task_name + \" | \" + str(expected_hours) + \" | \" + project_task_hours)\n",
    "  \n",
    "  df = spark.createDataFrame([(id, task_name, project_task_hours, expected_hours)], [\"ID_Tarefa\", \"Nome_Tarefa\", \"Projeto_Funcionario_Horas\", \"Horas_Previstas_Tarefa\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_TAREFA]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "df = spark.createDataFrame([(0, 'Não Definido', 'Não Definido', 0)], [\"ID_Tarefa\", \"Nome_Tarefa\", \"Projeto_Funcionario_Horas\", \"Horas_Previstas_Tarefa\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_TAREFA]\", mode=\"append\", properties=connectionProperties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd590b8e-7678-478d-a1c0-9d56181ed7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6964dcff-dc40-4750-9f08-1ba66ed01456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "projects = spark.read.jdbc(url=jdbcUrl, table=\"(select P.[CodigoProjeto], [Departamento], [Area], sum(H.[NHoras]) as [HorasPrevistas] from [stg].[EXT_TBL_PROJETOS] as P join [stg].[EXT_TBL_HORASPREVISTAS] as H on P.[CodigoProjeto] = H.[CodigoProjeto] group by P.[CodigoProjeto], [Departamento],[Area]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "''' for better vizualization\n",
    "select P.[CodigoProjeto], [Departamento], [Area], sum(H.[NHoras]) as [HorasPrevistas]\n",
    "from [stg].[EXT_TBL_PROJETOS] as P join [stg].[EXT_TBL_HORASPREVISTAS] as H\n",
    "on P.[CodigoProjeto] = H.[CodigoProjeto]\n",
    "group by P.[CodigoProjeto], [Departamento],[Area]\n",
    "'''\n",
    "\n",
    "# project not defined for tasks without project id\n",
    "\n",
    "id = 0\n",
    "project_code = \"NAO.DEFINIDO\"\n",
    "project_name = \"Projeto \" + project_code\n",
    "department = \"Não Especificado\"\n",
    "area = \"Não Especificada\"\n",
    "expected_hours = 0\n",
    "#print(str(id) + \" | \" + project_code + \" | \" + project_name + \" | \" + department + \" | \" + area + \" | \" + str(expected_hours) + \" horas previstas\")\n",
    "\n",
    "df = spark.createDataFrame([(id, project_code, project_name, department, area, expected_hours)], [\"ID_Projeto\", \"Codigo_Projeto\", \"Nome_Projeto\", \"Departamento\", \"Area\", \"Horas_Previstas_Projeto\"])\n",
    "df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PROJETO]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "\n",
    "i = 1\n",
    "\n",
    "for project in projects:\n",
    "  id = i\n",
    "  \n",
    "  project_code = project[0] #CodigoProjeto\n",
    "  \n",
    "  project_name = \"Projeto \" + project_code\n",
    "\n",
    "  if project[1] == None:\n",
    "    department = \"Não Especificado\"\n",
    "  else:\n",
    "    department = project[1]\n",
    "\n",
    "  if project[2] == None:\n",
    "    area = \"Não Especificada\"\n",
    "  else:\n",
    "    area = project[2]\n",
    "  \n",
    "  expected_hours = project[3]  #NHoras from the join of PROJETOS with HORASPREVISTAS\n",
    "\n",
    "  print(str(id) + \" | \" + project_code + \" | \" + project_name + \" | \" + department + \" | \" + area + \" | \" + str(expected_hours) + \" horas previstas\")\n",
    "\n",
    "  df = spark.createDataFrame([(id, project_code, project_name, department, area, expected_hours)], [\"ID_Projeto\", \"Codigo_Projeto\", \"Nome_Projeto\", \"Departamento\", \"Area\", \"Horas_Previstas_Projeto\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[DIM_PROJETO]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "  i += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f2ea917-ace3-4087-878a-3a37141c9f06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383b81ed-c381-4e3d-bfaf-e18730c04d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "imputations = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "'''for better visualization\n",
    "select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as horas_realizadas\n",
    "from [stg].[EXT_TBL_IMPUTACAO_DETALHE]\n",
    "group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]\n",
    "'''\n",
    "\n",
    "i = 0\n",
    "for imputation in imputations:\n",
    "  # Collected Data:\n",
    "  # [CodigoProjeto]   --imputation[0]\n",
    "  # [Username]        --imputation[1]\n",
    "  # [Ano]             --imputation[2]\n",
    "  # [Mes]             --imputation[3]\n",
    "  # [FK_TarefaID]     --imputation[4]\n",
    "  # HorasRealizadas   --imputation[5]\n",
    "\n",
    "  # Pretended Result:\n",
    "  # [ID] [int] PRIMARY KEY,\n",
    "\t# [Horas_Realizadas] [int],\n",
    "\t# [ID_Projeto] [int],\n",
    "\t# [ID_Calendario] [int],\n",
    "\t# [ID_Tarefa] [int],\n",
    "\t# [ID_Funcionario] [int],\n",
    "  # [ID_Perfil] [int],\n",
    "\t# [ID_Classificacao_Produtividade_Tarefa] [int]\n",
    "\n",
    "  # ID\n",
    "  id = i\n",
    "\n",
    "\n",
    "  # Hours Performed\n",
    "  hours_performed = imputation[5]\n",
    "  \n",
    "\n",
    "  # Project\n",
    "  query = \"(select [ID_Projeto] from [dwProdutividade].[DIM_PROJETO] where [Codigo_Projeto] = '\" + imputation[0] + \"') AS query\"\n",
    "  project = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "  if not project:\n",
    "    project_id = 0\n",
    "  else:\n",
    "    project_id = project[0][0]\n",
    "\n",
    "\n",
    "  # Calendar\n",
    "  if not imputation[2] or not imputation[3]:\n",
    "    calendar_id = 0\n",
    "  else:\n",
    "    query = \"(select [ID_Calendario] from [dwProdutividade].[DIM_CALENDARIO] where [Mes] = \" + str(imputation[3]) + \" and [Ano] = \" + str(imputation[2]) + \") AS query\"\n",
    "    calendar = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "    if not calendar:\n",
    "      calendar_id = 0\n",
    "    else:\n",
    "      calendar_id = calendar[0][0]\n",
    "\n",
    "\n",
    "  # Task\n",
    "  if not imputation[4]:\n",
    "    # go to the tasks table and look for the id of the task with the same CodigoProjeto and the same Username\n",
    "    query = \"(select [Tarefa] from [stg].[EXT_TBL_HORASPREVISTAS] where [CodigoProjeto] = '\" + imputation[0] + \"' and [Username] = '\" + imputation[1] + \"') AS query\"\n",
    "    task_info = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "    if not task_info:\n",
    "      task_id = 0\n",
    "      query = \"(select [ID_Tarefa], [Horas_Previstas_Tarefa], [Nome_Tarefa] from [dwProdutividade].[DIM_TAREFA] where [ID_Tarefa] = \" + str(task_id) + \") AS query\"\n",
    "      task = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()  # to be used in the productivity classification calculation\n",
    "    else:\n",
    "      query = \"(select [ID_Tarefa], [Horas_Previstas_Tarefa], [Nome_Tarefa] from [dwProdutividade].[DIM_TAREFA] where [Nome_Tarefa] = '\" + task_info[0][0] + \"') AS query\"\n",
    "      task = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "      task_id = task[0][0]\n",
    "  else:\n",
    "    task_id = imputation[4] # check if it works well, if not use above process\n",
    "    query = \"(select [ID_Tarefa], [Horas_Previstas_Tarefa], [Nome_Tarefa] from [dwProdutividade].[DIM_TAREFA] where [ID_Tarefa] = \" + str(task_id) + \") AS query\"\n",
    "    task = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()  # to be used in the productivity classification calculation\n",
    "\n",
    "\n",
    "  # Employee\n",
    "  query = \"(select [ID_Funcionario] from [dwProdutividade].[DIM_FUNCIONARIO] where [Nome_Funcionario] = '\" + imputation[1] + \"') AS query\"\n",
    "  employee = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "  if not employee:\n",
    "    employee_id = 0\n",
    "  else:\n",
    "    employee_id = employee[0][0]\n",
    "\n",
    "\n",
    "  # Profile\n",
    "  if not task:\n",
    "    profile_id = 0\n",
    "  else:\n",
    "    #query = \"(select distinct [Perfil] from [stg].[EXT_TBL_ORCAMENTO] where [CodigoProjeto] = '\" + imputation[0] + \"' and [Username] = '\" + imputation[1] + \"') AS query\"\n",
    "    query = \"(select [Perfil] from [stg].[EXT_TBL_HORASPREVISTAS] as H join [stg].[EXT_TBL_ORCAMENTO] as O on H.[OrcamentoID] = O.[ID] where H.[CodigoProjeto] = '\" + imputation[0] + \"' and [Username] = '\" + imputation[1] + \"' and H.[Tarefa] = '\" + task[0][2] + \"') AS query\"\n",
    "    #task[0][2] -> Nome Tarefa == Tarefa em Horas Previstas\n",
    "    profile_info = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "    if not profile_info:\n",
    "      profile_id = 0\n",
    "    else:\n",
    "      query = \"(select [ID_Perfil] from [dwProdutividade].[DIM_PERFIL] where [Nome_Perfil] = '\" + profile_info[0][0] + \"') AS query\"\n",
    "      profile = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "      if not profile:\n",
    "        profile_id = 0\n",
    "      else:\n",
    "        profile_id = profile[0][0]\n",
    "\n",
    "  ''' for better visualization\n",
    "  select [Perfil]\n",
    "  from [stg].[EXT_TBL_HORASPREVISTAS] as H join [stg].[EXT_TBL_ORCAMENTO] as O\n",
    "  on H.[OrcamentoID] = O.[ID]\n",
    "  where H.[CodigoProjeto] = '\" + imputation[0] + \"' and [Username] = '\" + imputation[1] + \"' and H.[Tarefa] = '\" + task[0][2] + \"'\"\n",
    "  # where H.[CodigoProjeto] = 'DGIE.2023.304' and [Username] = 'utilizador.355' and H.[Tarefa] = 'Tarefa - Utilizador #355'\n",
    "  '''\n",
    "\n",
    "\n",
    "  # Productivity Classification - in a global perspective, not in a month related one\n",
    "  '''\n",
    "  [0%, 100%[ - 5        - classification id 0\n",
    "  100% - 4              - classification id 1\n",
    "  ]100%, 125%] - 3      - classification id 2\n",
    "  ]125%, 150%] - 2      - classification id 3\n",
    "  ]150%, 175%] - 1      - classification id 4\n",
    "  ]175%, +∞] - 0        - classification id 5\n",
    "  '''\n",
    "\n",
    "  if not task: #or expected == 0:\n",
    "    classification_id = 6\n",
    "  else:\n",
    "    expected = task[0][1] \n",
    "    #realized = imputation[5] - monthly\n",
    "    query = \"(select [CodigoProjeto], [Username], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] where [CodigoProjeto] = '\" + imputation[0] + \"' and [Username] = '\" + imputation[1] + \"' and [FK_TarefaID] = \" + str(task_id) + \" group by [CodigoProjeto], [Username], [FK_TarefaID]) AS query\"\n",
    "    realized = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "\n",
    "    # verify if the condition of FK_TarefaID works\n",
    "\n",
    "    ''' for better visualization\n",
    "    select [CodigoProjeto], [Username], [FK_TarefaID], sum([NHoras]) as HorasRealizadas \n",
    "    from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \n",
    "    group by [CodigoProjeto], [Username], [FK_TarefaID]\n",
    "    order by [CodigoProjeto], [Username]\n",
    "    '''\n",
    "\n",
    "    ''' this query is for a month related calculus, if it was needed the hours realized in a certain month\n",
    "    select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas \n",
    "    from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \n",
    "    group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]\n",
    "    where [CodigoProjeto] = imputation[0] and [Username] = imputation[1] and [Ano] = str(imputation[2]) and [Mes] = str(imputation[3]) and [FK_TarefaID] = str(task_id)\n",
    "    '''\n",
    "\n",
    "    if expected == 0:\n",
    "      classification_id = 6   # if it is not a value for hours expected\n",
    "    else:\n",
    "      if not realized:\n",
    "        classification_id = 6\n",
    "      else:\n",
    "        quotient = realized[0][3] / expected #realized[0][3] is the realized hours sum column\n",
    "        print(quotient)\n",
    "\n",
    "        if quotient < 1:\n",
    "          classification_id = 0\n",
    "        elif quotient == 1:\n",
    "          classification_id = 1\n",
    "        elif quotient <= 1.25:\n",
    "          classification_id = 2\n",
    "        elif quotient <= 1.50:\n",
    "          classification_id = 3\n",
    "        elif quotient <= 1.75:\n",
    "          classification_id = 4\n",
    "        else: \n",
    "          classification_id = 5\n",
    "\n",
    "  df = spark.createDataFrame([(id, hours_performed, project_id, calendar_id, task_id, employee_id, profile_id, classification_id)], [\"ID\", \"Horas_Realizadas\", \"ID_Projeto\", \"ID_Calendario\", \"ID_Tarefa\", \"ID_Funcionario\", \"ID_Perfil\", \"ID_Classificacao_Produtividade_Tarefa\"])\n",
    "  df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[FACTO_TAREFA]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "  print(str(id) + \" | \" + str(hours_performed) + \" horas realizadas | \" +  str(project_id) + \" | \" + str(calendar_id) + \" | \" + str(task_id) + \" | \" + str(employee_id) + \" | \" + str(profile_id) + \" | \" + str(classification_id))\n",
    "\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4fa1c6-79c7-4dbf-9e18-12fc7541d09a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "'''\n",
    "code = 'ABR.2016.016'\n",
    "query = \"(select [CodigoProjeto], [Username], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] where [CodigoProjeto] = '\" + code + \"' and [Username] = 'utilizador.211' and [FK_TarefaID] = 0 group by [CodigoProjeto], [Username], [FK_TarefaID]) AS query\"\n",
    "realized = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "\n",
    "print(realized[0][0])\n",
    "print(realized[0][1])\n",
    "print(realized[0][2])\n",
    "print(realized[0][3])'''\n",
    "'''\n",
    "query = \"(select [ID_Calendario] from [dwProdutividade].[DIM_CALENDARIO] where [Mes] = 5 and [Ano] = 2017) AS query\"\n",
    "calendar = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "print(calendar[0][0])'''\n",
    "'''\n",
    "query = \"(select [ID_Funcionario] from [dwProdutividade].[DIM_FUNCIONARIO] where [Nome_Funcionario] = 'utilizador.213') AS query\"\n",
    "employee = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "print(str(employee[0][0]))'''\n",
    "\n",
    "query = \"(select [Perfil] from [stg].[EXT_TBL_HORASPREVISTAS] as H join [stg].[EXT_TBL_ORCAMENTO] as O on H.[OrcamentoID] = O.[ID] where H.[CodigoProjeto] = 'DGIE.2023.304' and [Username] = 'utilizador.355' and H.[Tarefa] = 'Tarefa - Utilizador #355') AS query\"\n",
    "profile_info = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "print(profile_info[0][0])\n",
    "print(profile_info[0][0] == None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "575a492b-a23f-4851-974d-3fe05584da38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eb8b41-dc18-4dc7-bdbd-1a4e90da19b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# Total hours executed in each project in each month\n",
    "\n",
    "projects_mensal = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Ano], [Mes], sum([HorasRealizadas]) as TotalHorasRealizadas from ( select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T group by [CodigoProjeto], [Ano], [Mes]) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "''' for better visualization\n",
    "select [CodigoProjeto], [Ano], [Mes], sum([HorasRealizadas]) as TotalHorasRealizadas\n",
    "from (\n",
    "    select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas \n",
    "from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \n",
    "group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T\n",
    "group by [CodigoProjeto], [Ano], [Mes]\n",
    "'''\n",
    "\n",
    "i = 0\n",
    "\n",
    "for project in projects_mensal:\n",
    "\n",
    "  # Collected Data:\n",
    "  # [CodigoProjeto]   --project[0]\n",
    "  # [Ano]             --project[1]\n",
    "  # [Mes]             --project[2]\n",
    "  # HorasRealizadas   --project[3]\n",
    "\n",
    "  # Pretended Result:\n",
    "  # [ID] [int] PRIMARY KEY,\n",
    "  # [Avanco] [int],\n",
    "  # [Horas_Realizadas] [int],\n",
    "  # [ID_Estado] [int],\n",
    "  # [ID_Projeto] [int],\n",
    "  # [ID_Calendario] [int],\n",
    "  # [ID_Classificacao_Produtividade_Tarefa] [int]\n",
    "\n",
    "  # ID\n",
    "  id = i\n",
    "\n",
    "\n",
    "  # Advance\n",
    "  advance = 0\n",
    "  month = project[2]\n",
    "  year = project[1]\n",
    "  min_year = 2010\n",
    "  condition = True\n",
    "  while year >= 2010 and condition:\n",
    "    while month >= 1 and condition:\n",
    "      query = \"(select [Avanco] from [stg].[EXT_TBL_HISTORICO_AVANCOS] where [CodigoProjecto] = '\" + project[0] + \"' and MONTH(DataAvanco) = '\" + str(month) + \"' and YEAR(DataAvanco) = '\" + str(year) + \"') AS query\"\n",
    "      advance_info = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "\n",
    "      if advance_info:\n",
    "        advance = advance_info[0][0]\n",
    "        condition = False\n",
    "\n",
    "      month -= 1\n",
    "    year -= 1\n",
    "\n",
    "\n",
    "  # Hours Performed\n",
    "  hours_performed = project[3]\n",
    "\n",
    "\n",
    "  # State\n",
    "  # it is a fixed value, it is available only on stg.EXT_TBL_PROJETOS table\n",
    "  query = \"(select [Estado] from [stg].[EXT_TBL_PROJETOS] where [CodigoProjeto] = '\" + project[0] + \"') AS query\"\n",
    "  state = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "  if not state:\n",
    "    state_id = 0   # Undefined state for exceptional cases\n",
    "  else:\n",
    "    state_id = state[0][0]\n",
    "\n",
    "\n",
    "  # Project\n",
    "  query = \"(select [ID_Projeto], [Horas_Previstas_Projeto] from [dwProdutividade].[DIM_PROJETO] where [Codigo_Projeto] = '\" + project[0] + \"') AS query\"\n",
    "  project_info = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()      # also useful for productivity classification calculus\n",
    "  if not project_info:\n",
    "    project_id = 0\n",
    "  else:\n",
    "    project_id = project_info[0][0]\n",
    "\n",
    "\n",
    "  # Calendar\n",
    "  if not project[1] or not project[2]:    # Year ; Month\n",
    "    calendar_id = 0\n",
    "  else:\n",
    "    query = \"(select [ID_Calendario] from [dwProdutividade].[DIM_CALENDARIO] where [Mes] = \" + str(project[2]) + \" and [Ano] = \" + str(project[1]) + \") AS query\"\n",
    "    calendar = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "    if not calendar:\n",
    "      calendar_id = 0\n",
    "    else:\n",
    "      calendar_id = calendar[0][0]\n",
    "\n",
    "\n",
    "  # Productivity Classification\n",
    "  query = \"(select sum([HorasRealizadas]) as TotalHorasRealizadas from ( select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas from [stg].[EXT_TBL_IMPUTACAO_DETALHE] group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T where [CodigoProjeto] = '\" + project[0] + \"' group by [CodigoProjeto]) AS query\"\n",
    "  realized = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "\n",
    "  ''' for better visualization\n",
    "  select sum([HorasRealizadas]) as TotalHorasRealizadas\n",
    "  from (\n",
    "      select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as HorasRealizadas \n",
    "  from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \n",
    "  group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID]) as T\n",
    "  where [CodigoProjeto] = '\" + project[0] + \"'\n",
    "  group by [CodigoProjeto]\n",
    "  '''\n",
    "\n",
    "  if not realized:\n",
    "    classification_id = 6\n",
    "  else:\n",
    "    #query = \"(select [Horas_Previstas_Projeto] from [dwProdutividade].[DIM_PROJETO] where [Codigo_Projeto] = '\" + project[0] + \"') AS query\"\n",
    "    #expected = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties).collect()\n",
    "    #if not expected:\n",
    "    if not project_info:       # declared before with the needed info\n",
    "      classification_id = 6\n",
    "    else:\n",
    "      if project_info[0][1] == 0:\n",
    "        classification_id = 6\n",
    "      else:\n",
    "        quotient = realized[0][0] / project_info[0][1]\n",
    "        print(quotient)\n",
    "\n",
    "        if quotient < 1:\n",
    "          classification_id = 0\n",
    "        elif quotient == 1:\n",
    "          classification_id = 1\n",
    "        elif quotient <= 1.25:\n",
    "          classification_id = 2\n",
    "        elif quotient <= 1.50:\n",
    "          classification_id = 3\n",
    "        elif quotient <= 1.75:\n",
    "          classification_id = 4\n",
    "        else: \n",
    "          classification_id = 5\n",
    "\n",
    "\n",
    "  df = spark.createDataFrame([(id, advance, hours_performed, state_id, project_id, calendar_id, classification_id)], [\"ID\", \"Avanco_Projeto\", \"Horas_Realizadas_Projeto\", \"ID_Estado\", \"ID_Projeto\", \"ID_Calendario\", \"ID_Classificacao_Produtividade_Projeto\"])\n",
    "  #df.write.jdbc(url=jdbcUrl, table=\"[dwProdutividade].[FACTO_PROJETO]\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "  print(str(id) + \" | \" + str(advance) + \"% | \" + str(hours_performed) + \" horas realizadas | \" + str(state_id) + \" | \" +  str(project_id) + \" | \" + str(calendar_id) + \" | \" + str(classification_id))\n",
    "\n",
    "  i += 1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4084851272068028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_transformation_notebook",
   "notebookOrigID": 4084851272068023,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
