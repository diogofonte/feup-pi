{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f5abf6-e9e3-4f30-bd72-fd95a2bc3eca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connection to the Azure SQL Database\n",
    "\n",
    "Defined some variables to programmatically create the connection to the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a839980-6acf-4e62-96f7-c842a51bb1e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUsername = \"feuplogin\"\n",
    "jdbcPassword = \"Logproject33\"\n",
    "jdbcHostname = \"intranetfeupserver.database.windows.net\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"intranet14\"\n",
    "\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\": jdbcUsername,\n",
    "  \"password\": jdbcPassword,\n",
    "  \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f477a22c-52ac-4054-a9d3-70cf2da82c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Transformation Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61d58e1-09e5-46d2-b627-ec4dffc11163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To test the data transformation executed, I reduced the dataset to the project AGEAS.2017.190 (ID: 155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901047e2-da39-446c-bc4e-81eb2e1acee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PROJECT FACT\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "# compare hours planned\n",
    "hours_planned_stg = spark.read.jdbc(url=jdbcUrl, table=\"(select sum(H.[NHoras]) as [HorasPrevistas] \\\n",
    "                                                            from [stg].[EXT_TBL_PROJETOS] as P \\\n",
    "                                                            join [stg].[EXT_TBL_HORASPREVISTAS] as H on P.[CodigoProjeto] = H.[CodigoProjeto] \\\n",
    "                                                            where P.[CodigoProjeto] = 'AGEAS.2017.190' \\\n",
    "                                                            group by P.[CodigoProjeto]) AS query\", properties=connectionProperties).collect()\n",
    "hours_planned_dw = spark.read.jdbc(url=jdbcUrl, table=\"(select Horas_Previstas_Projeto from [dwProdutividade].[DIM_PROJETO] where Codigo_Projeto = 'AGEAS.2017.190') AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "print(\"HOURS PLANNED - AGEAS.2017.190\")\n",
    "print(\"Hours Planned STG: \" + str(hours_planned_stg[0][0]))\n",
    "print(\"Hours Planned DW: \" + str(hours_planned_dw[0][0]))\n",
    "#print(hours_planned_stg[0][0] == hours_planned_dw[0][0])\n",
    "\n",
    "print('----------------------------------------------\\n')\n",
    "\n",
    "# compare total hours executed\n",
    "hours_performed_stg = spark.read.jdbc(url=jdbcUrl, table=\"(select sum([NHoras]) as Total_Horas_Realizadas_Projeto \\\n",
    "                                                            from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \\\n",
    "                                                            where [CodigoProjeto] = 'AGEAS.2017.190' \\\n",
    "                                                            group by [CodigoProjeto]) AS query\", properties=connectionProperties).collect()\n",
    "hours_performed_dw = spark.read.jdbc(url=jdbcUrl, table=\"(select sum([Horas_Realizadas_Projeto]) as Total_Horas_Realizadas_Projeto \\\n",
    "                                                            from [dwProdutividade].[FACTO_PROJETO] \\\n",
    "                                                            where [ID_Projeto] = 155) AS query\", properties=connectionProperties).collect()\n",
    "\n",
    "print(\"TOTAL HOURS PERFORMED - AGEAS.2017.190\")\n",
    "print(\"Hours PERFORMED STG: \" + str(hours_performed_stg[0][0]))\n",
    "print(\"Hours PERFORMED DW: \" + str(hours_performed_dw[0][0]))\n",
    "#print(hours_planned_stg[0][0] == hours_planned_dw[0][0])\n",
    "\n",
    "print('----------------------------------------------\\n')\n",
    "\n",
    "# Imputations - By Project\n",
    "\n",
    "print(\"IMPUTACAO_DETALHE\")\n",
    "imputations_dw = spark.read.jdbc(url=jdbcUrl, table=\"(select IMP.CodigoProjeto, IMP.Ano, IMP.Mes, IMP.SumNHoras as Horas_Realizadas, HA.Avanco, IMP.Estado \\\n",
    "                                                        from \\\n",
    "                                                            (select IM.CodigoProjeto as CodigoProjeto, IM.Ano as Ano, IM.Mes as Mes, sum(IM.NHoras) as SumNHoras, P.Estado as Estado \\\n",
    "                                                            from stg.EXT_TBL_IMPUTACAO_DETALHE as IM, stg.EXT_TBL_PROJETOS as P \\\n",
    "                                                            where IM.CodigoProjeto = 'AGEAS.2017.190' and IM.CodigoProjeto = P.CodigoProjeto \\\n",
    "                                                            group by IM.CodigoProjeto, IM.Ano, IM.Mes, P.Estado) as IMP \\\n",
    "                                                        left join stg.EXT_TBL_HISTORICO_AVANCOS as HA \\\n",
    "                                                        on IMP.CodigoProjeto = HA.CodigoProjecto and Ano = YEAR(DataAvanco) and Mes = MONTH(DataAvanco) \\\n",
    "                                                        group by IMP.CodigoProjeto, IMP.Ano, IMP.Mes, IMP.SumNHoras, HA.Avanco, IMP.Estado \\\n",
    "                                                        order by IMP.Ano ASC, IMP.Mes ASC) AS query\", properties=connectionProperties)\n",
    "\n",
    "print(\"FACTO PROJETO\")\n",
    "imputations_dw = spark.read.jdbc(url=jdbcUrl, table=\"(select Codigo_Projeto, ID_Calendario, Horas_Realizadas_Projeto as Horas_Realizadas, Avanco_Projeto as Avanco, \\\n",
    "                                                        Horas_Previstas_Mes_Por_Avanco, Horas_Previstas_Projeto\\\n",
    "                                                        from dwProdutividade.FACTO_PROJETO as FP, dwProdutividade.DIM_PROJETO as P, dwProdutividade.DIM_ESTADO as E \\\n",
    "                                                        where FP.ID_Projeto = 155 \\\n",
    "                                                            and FP.ID_Projeto = P.ID_Projeto \\\n",
    "                                                            and FP.ID_Estado = E.ID_Estado) AS query\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa7362a-d687-43b3-b955-c1d1012e1d75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK FACT\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "\n",
    "# compare hours planned\n",
    "hours_planned_stg = spark.read.jdbc(url=jdbcUrl, table=\"(select [ID], [CodigoProjeto], [Username], [Tarefa], [NHoras] \\\n",
    "                                                            from [stg].[EXT_TBL_HORASPREVISTAS] \\\n",
    "                                                            where CodigoProjeto = 'AGEAS.2017.190') AS query\", properties=connectionProperties)\n",
    "hours_planned_dw = spark.read.jdbc(url=jdbcUrl, table=\"(select ID_Projeto, FT.ID_Tarefa, Horas_Previstas_Tarefa \\\n",
    "                                                            from dwProdutividade.Facto_Tarefa as FT, dwProdutividade.DIM_TAREFA as DT \\\n",
    "                                                            where ID_Projeto = 155 and FT.ID_Tarefa = DT.ID_Tarefa \\\n",
    "                                                            group by ID_Projeto, FT.ID_Tarefa, Horas_Previstas_Tarefa) AS query\", properties=connectionProperties)\n",
    "\n",
    "imputations_proof = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Username], [FK_TarefaID], sum([NHoras]) as Horas_Realizadas_Tarefa \\\n",
    "                                                            from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \\\n",
    "                                                            where CodigoProjeto = 'AGEAS.2017.190' \\\n",
    "                                                            group by [CodigoProjeto], [Username], [FK_TarefaID]) AS query\", properties=connectionProperties)\n",
    "\n",
    "print(\"HOURS PLANNED - AGEAS.2017.190 TASKS\")\n",
    "print(\"Hours Planned STG:\\n\")\n",
    "hours_planned_stg.show()\n",
    "print(\"Hours Planned DW: \\n\")\n",
    "hours_planned_dw.show()\n",
    "print(\"Proof that the FKTarefaID is null: \\n\")\n",
    "imputations_proof.show()\n",
    "\n",
    "print('----------------------------------------------\\n')\n",
    "\n",
    "# Imputations - By Task\n",
    "hours_performed_stg = spark.read.jdbc(url=jdbcUrl, table=\"(select [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID], sum([NHoras]) as Horas_Realizadas_Tarefa \\\n",
    "                                                            from [stg].[EXT_TBL_IMPUTACAO_DETALHE] \\\n",
    "                                                            where CodigoProjeto = 'AGEAS.2017.190' \\\n",
    "                                                            group by [CodigoProjeto], [Username], [Ano], [Mes], [FK_TarefaID] \\\n",
    "                                                            order by [Ano] ASC, [Mes] ASC) AS query\", properties=connectionProperties)\n",
    "\n",
    "query = \"(select Codigo_Projeto, Nome_Funcionario, ID_Calendario, ID_Tarefa, Horas_Realizadas_Tarefa, ID_Perfil, ID_Classificacao_Produtividade_Tarefa \\\n",
    "from dwProdutividade.FACTO_TAREFA AS FT, dwProdutividade.DIM_PROJETO AS DP, dwProdutividade.DIM_FUNCIONARIO as DF \\\n",
    "where FT.ID_Projeto = 155 and FT.ID_Projeto = DP.ID_Projeto and FT.ID_Funcionario = DF.ID_Funcionario \\\n",
    "order by ID_Calendario ASC) AS query\"\n",
    "hours_performed_dw = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties)\n",
    "\n",
    "hours_performed_stg.show()\n",
    "hours_performed_dw.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_transformation_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
