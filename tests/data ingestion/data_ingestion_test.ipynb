{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f5abf6-e9e3-4f30-bd72-fd95a2bc3eca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connection to the Azure SQL Database\n",
    "\n",
    "Defined some variables to programmatically create the connection to the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a839980-6acf-4e62-96f7-c842a51bb1e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUsername = \"feuplogin\"\n",
    "jdbcPassword = \"Logproject33\"\n",
    "jdbcHostname = \"intranetfeupserver.database.windows.net\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"intranet14\"\n",
    "\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\": jdbcUsername,\n",
    "  \"password\": jdbcPassword,\n",
    "  \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f477a22c-52ac-4054-a9d3-70cf2da82c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Ingestion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6b0b9b-1640-4b17-a546-3d99ce5bda41",
     "showTitle": true,
     "title": "HISTORICO_AVANCOS"
    }
   },
   "outputs": [],
   "source": [
    "# HISTORICO_AVANCOS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "dbo_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [dbo].[TBL_HISTORICO_AVANCOS]) AS query\", properties=connectionProperties)\n",
    "stg_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [stg].[EXT_TBL_HISTORICO_AVANCOS]) AS query\", properties=connectionProperties)\n",
    "\n",
    "dbo = dbo_table.toPandas()\n",
    "stg = stg_table.toPandas()\n",
    "\n",
    "print(dbo.equals(stg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901047e2-da39-446c-bc4e-81eb2e1acee5",
     "showTitle": true,
     "title": "HORASPREVISTAS"
    }
   },
   "outputs": [],
   "source": [
    "# HORASPREVISTAS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "dbo_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [dbo].[TBL_HORASPREVISTAS]) AS query\", properties=connectionProperties)\n",
    "stg_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [stg].[EXT_TBL_HORASPREVISTAS]) AS query\", properties=connectionProperties)\n",
    "\n",
    "dbo = dbo_table.toPandas()\n",
    "stg = stg_table.toPandas()\n",
    "\n",
    "print(dbo.equals(stg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf07001-6089-498e-bb86-17cf064fb12e",
     "showTitle": true,
     "title": "IMPUTACAO_DETALHE"
    }
   },
   "outputs": [],
   "source": [
    "# IMPUTACAO_DETALHE\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "dbo_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [dbo].[TBL_IMPUTACAO_DETALHE]) AS query\", properties=connectionProperties)\n",
    "stg_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [stg].[EXT_TBL_IMPUTACAO_DETALHE]) AS query\", properties=connectionProperties)\n",
    "\n",
    "dbo = dbo_table.toPandas()\n",
    "stg = stg_table.toPandas()\n",
    "\n",
    "print(dbo.equals(stg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3798917-5511-4961-8fa1-82ebb65c9a37",
     "showTitle": true,
     "title": "ORCAMENTO"
    }
   },
   "outputs": [],
   "source": [
    "# ORCAMENTO\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "dbo_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [dbo].[TBL_ORCAMENTO]) AS query\", properties=connectionProperties)\n",
    "stg_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [stg].[EXT_TBL_ORCAMENTO]) AS query\", properties=connectionProperties)\n",
    "\n",
    "dbo = dbo_table.toPandas()\n",
    "stg = stg_table.toPandas()\n",
    "\n",
    "print(dbo.equals(stg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84361a1a-55c9-49a9-82f3-ca2e75f71b64",
     "showTitle": true,
     "title": "PROJETOS"
    }
   },
   "outputs": [],
   "source": [
    "# PROJETOS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Produtividade\").getOrCreate()\n",
    "\n",
    "dbo_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [dbo].[TBL_PROJETOS]) AS query\", properties=connectionProperties)\n",
    "stg_table = spark.read.jdbc(url=jdbcUrl, table=\"(select * from [stg].[EXT_TBL_PROJETOS]) AS query\", properties=connectionProperties)\n",
    "\n",
    "dbo = dbo_table.toPandas()\n",
    "stg = stg_table.toPandas()\n",
    "\n",
    "print(dbo.equals(stg))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_ingestion_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
